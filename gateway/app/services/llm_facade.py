# app/services/llm_facade.py
from .llm_port import LLMClient
from .gemini_adapter import GeminiAdapter
from .retry_decorator import RetryDecorator
from .llm_router import LLMRouter

# Ğ¯ĞºÑ‰Ğ¾ Ñ…Ğ¾Ñ‡ĞµÑˆ fallback, ĞºĞ¾Ğ»Ğ¸ Ğ±ÑƒĞ´Ğµ OpenAIAdapter:
# from .openai_adapter import OpenAIAdapter

class LLMFacade:
    def __init__(self, client: LLMClient | None = None):
        # ğŸ”¹ Ğ‘Ğ°Ğ·Ğ¾Ğ²Ğ¸Ğ¹ ĞºĞ»Ñ–Ñ”Ğ½Ñ‚
        base_client = client or GeminiAdapter()

        # ğŸ”¹ ĞĞ±Ğ³Ğ¾Ñ€Ñ‚ĞºĞ° Ğ· Ñ€ĞµÑ‚Ñ€Ğ°ÑĞ¼Ğ¸
        retry_client = RetryDecorator(base_client, retries=2, delay=1.0)

        # ğŸ”¹ Ğ¯ĞºÑ‰Ğ¾ Ğ´Ğ¾Ğ´Ğ°Ğ¼Ğ¾ fallback (Ğ½Ğ°Ğ¿Ñ€. OpenAI)
        # fallback_client = OpenAIAdapter()
        # self.client = LLMRouter(retry_client, fallback_client)
        # ĞŸĞ¾ĞºĞ¸ Ñ‰Ğ¾ Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ retry
        self.client = retry_client

    def complete(self, messages, **kw) -> str:
        return next(self.client.chat_stream(messages, **kw))

    def complete_stream(self, messages, **kw):
        return self.client.chat_stream(messages, **kw)
